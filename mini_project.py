# -*- coding: utf-8 -*-
"""Mini Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tVeqWLeM6iKMwh5odAYfR3OyU4EG1P_l

**1.Bussiness Problem**
The data scientists at BigMart have collected 2013 sales data for 1559 products across 10 stores in different cities. Also, certain attributes of each product and store have been defined. The aim of this data science project is to build a predictive model and find out the sales of each product at a particular store.
"""

# Loading useful libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import StandardScaler

"""**2. The Data Set**"""

# Loading data
from google.colab import files
uploaded = files.upload()

import io
raw_data = pd.read_csv(io.BytesIO(uploaded['Train.csv']))

raw_data.head()

# Checking Data
raw_data.describe(include='all')

# Checking missing values

raw_data.isna().sum()

# Calculating missing value percentage
print('Item_Weight missing values:' ,raw_data.Item_Weight.isna().sum() / len(raw_data.Item_Weight) * 100, '%')
print('Outlet_Size missing values:' ,raw_data.Outlet_Size.isna().sum() / len(raw_data.Item_Weight) * 100, '%')

# Filling missing data
 #for Outlet_Size
Outlet_Size_mode = raw_data.Outlet_Size.mode().values[0]
print(Outlet_Size_mode)

raw_data.Outlet_Size = raw_data.Outlet_Size.replace(np.nan,Outlet_Size_mode)

print(raw_data.shape)

raw_data.head()

# Eliminating missing data
 #for Item_Weight

raw_data.dropna(axis=0,inplace=True)

print(raw_data.shape)

# Checking Data Types
raw_data.info()

# Handling irregular texts
print(raw_data['Item_Fat_Content'].unique())
raw_data['Item_Fat_Content'] = raw_data['Item_Fat_Content'].str.lower()
raw_data['Item_Fat_Content'] = raw_data['Item_Fat_Content'].replace(to_replace='lf',value='low fat')
raw_data['Item_Fat_Content'] = raw_data['Item_Fat_Content'].replace(to_replace='reg',value='regular')
print(raw_data['Item_Fat_Content'].unique())

print(raw_data['Item_Type'].unique())

print(raw_data['Outlet_Identifier'].unique())

print(raw_data['Outlet_Location_Type'].unique())

print(raw_data['Outlet_Type'].unique())

raw_data.duplicated().sum()

"""**3. Exploratory Data Analysis (eda) - Outliers**"""

# Checking Outliers
col_list = ['Item_Weight','Item_Visibility','Item_MRP']

for i in col_list:
  raw_data.boxplot(column=i)
  plt.show()

# Item_Visibility has outliers
# Replacing the outlier values using the IQR

Q1,Q3 = raw_data['Item_Visibility'].quantile([0.25,0.75])
print('25% value: ', Q1,'75% value: ', Q3 )
IQR = Q3 - Q1
upper_limit = Q3+(1.5*IQR)
lower_limit = Q1-(1.5*IQR)
print('IQR value: ', IQR)
print('Upper Limit: ', upper_limit)
print('Lower Limit: ', lower_limit)
raw_data['Item_Visibility']=np.where(raw_data['Item_Visibility']>upper_limit,upper_limit,raw_data['Item_Visibility'])
raw_data['Item_Visibility']=np.where(raw_data['Item_Visibility']<lower_limit,lower_limit,raw_data['Item_Visibility'])
# checking outliers again
raw_data.boxplot(column='Item_Visibility')
plt.show()

# Correlation
raw_data.corr()

"""**4. Exploratory Data Analysis (eda) - Graphs**"""

col_list2 = ['Item_Weight','Item_Visibility','Item_MRP','Outlet_Establishment_Year']
for i in col_list2:
  sns.scatterplot(x=i,y='Item_Outlet_Sales',data=raw_data,ci=None)
  plt.show()

std_scale=StandardScaler()
std_scale

# Data Normalization
col_list3 = ['Item_Weight','Item_Visibility','Item_MRP','Item_Outlet_Sales']
for i in col_list3:
  raw_data[i] = std_scale.fit_transform(raw_data[[i]])
raw_data.head()
print(raw_data.shape)

## Removing 'Item_Identifier','Outlet_Identifier' columns from data
raw_data=raw_data.drop(['Item_Identifier','Outlet_Identifier'],axis=1)
raw_data.head()

print(raw_data.shape)

"""**5.Converting Categorical To Numerical**"""

# Encoding

to_encode = raw_data[['Item_Fat_Content','Item_Type','Outlet_Establishment_Year','Outlet_Size','Outlet_Location_Type','Outlet_Type']]
dummies = pd.get_dummies(raw_data[['Item_Fat_Content','Item_Type','Outlet_Establishment_Year','Outlet_Size','Outlet_Location_Type','Outlet_Type']],columns= ['Item_Fat_Content','Item_Type','Outlet_Establishment_Year','Outlet_Size','Outlet_Location_Type','Outlet_Type'],prefix = ['Item_Fat_Content','Item_Type','Outlet_Establishment_Year','Outlet_Size','Outlet_Location_Type','Outlet_Type'],drop_first=True).head()

columns= ['Item_Fat_Content','Item_Type','Outlet_Establishment_Year','Outlet_Size','Outlet_Location_Type','Outlet_Type']
raw_data = pd.concat([raw_data,dummies],axis=1)
raw_data.drop(columns,axis=1,inplace=True)

raw_data.head()

"""**6.Separating Training And Test Data**"""

## Separate feature columns and target column

X = raw_data.drop('Item_Outlet_Sales',axis=1)
y= raw_data['Item_Outlet_Sales']

# train_test_split is used to split the dataset into test set and train set
from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)

print("Original shape of dataset: {}".format(raw_data.shape)) # shape of original dataset
print("Shape of training dataset: {}".format(X_train.shape)) # shape of training dataset after split
print("Shape of testing dataset: {}".format(X_test.shape)) # shape of testing dataset after split

# Eliminating Nan Values
X_train.fillna(0,inplace=True)
X_test.fillna(0,inplace=True)

"""**7.Running The Models**"""

# Decision Tree Model

from sklearn.tree import DecisionTreeRegressor

# Define model. Specify a number for random_state to ensure same results each run
DT_model = DecisionTreeRegressor(max_leaf_nodes=50,random_state=1)

# Fit model
DT_model.fit(X_train, y_train)

# Predict
DT_predictions = DT_model.predict(X_test)

# Validate
print(mean_absolute_error(y_test, DT_predictions))

# Random Forest Model

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_absolute_error


# Define model. Specify a number for random_state to ensure same results each run
RF_model =  RandomForestRegressor(random_state=1)

# Fit model
RF_model.fit(X_train, y_train)

# Predict
predictions = RF_model.predict(X_test)

# Validate
print(mean_absolute_error(y_test, predictions))

# Linear Regressor Model

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score, mean_squared_error

# Define model. Specify a number for random_state to ensure same results each run
LR_model = LinearRegression()

# Fit model
LR_model.fit(X_train, y_train)

# Predict

LR_prediction= LR_model.predict(X_test)

# Validate
mse = mean_squared_error(y_test, LR_prediction)

r2 = r2_score(y_test, LR_prediction)#Best fit lineplt.scatter(x, y)

#Results
print("Mean Squared Error : ", mse)
print("R-Squared :" , r2)
print("Y-intercept :"  , LR_model.intercept_)
print("Slope :" , LR_model.coef_)

"""**8.Hyper Parameter Tuning XGB And GBR**"""

# XG Boost Model
from xgboost import XGBRegressor

XG_model = XGBRegressor(n_estimators=1000)
XG_model.fit(X_train, y_train,early_stopping_rounds=5, 
             eval_set=[(X_test, y_test)],
             verbose=False)
predictions = XG_model.predict(X_test)
print("Mean Absolute Error: " + str(mean_absolute_error(predictions, y_test)))

"""WORKING ON TEST DATA"""

# Loading test data
from google.colab import files
uploaded_test = files.upload()

import io
test_data = pd.read_csv(io.BytesIO(uploaded_test['Test.csv']))

print(test_data.head())

test_data.isna().sum()

# Filling missing test data 
 #for Outlet_Size
test_Outlet_Size_mode = test_data.Outlet_Size.mode().values[0]
test_data.Outlet_Size = test_data.Outlet_Size.replace(np.nan,test_Outlet_Size_mode)
 #for Item_Weight
test_data.dropna(axis=0,inplace=True)
test_data.isna().sum()

print(test_data.shape)

# Irregular text in test data
print(test_data['Item_Fat_Content'].unique())
test_data['Item_Fat_Content'] = test_data['Item_Fat_Content'].str.lower()
test_data['Item_Fat_Content'] = test_data['Item_Fat_Content'].replace(to_replace='lf',value='low fat')
test_data['Item_Fat_Content'] = test_data['Item_Fat_Content'].replace(to_replace='reg',value='regular')
print(test_data['Item_Fat_Content'].unique())

# Data Normalization in Test Data

test_data['Item_Weight'] = std_scale.fit_transform(test_data[['Item_Weight']])
test_data['Item_Visibility'] = std_scale.fit_transform(test_data[['Item_Visibility']])
test_data['Item_MRP'] = std_scale.fit_transform(test_data[['Item_MRP']])
print(test_data.head())

## Removing 'Item_Identifier','Outlet_Identifier' columns from test data
test_data=test_data.drop(['Item_Identifier','Outlet_Identifier'],axis=1)
test_data.head()

# Encoding test data

to_encode = test_data[['Item_Fat_Content','Item_Type','Outlet_Establishment_Year','Outlet_Size','Outlet_Location_Type','Outlet_Type']]
test_dummies = pd.get_dummies(test_data[['Item_Fat_Content','Item_Type','Outlet_Establishment_Year','Outlet_Size','Outlet_Location_Type','Outlet_Type']],columns= ['Item_Fat_Content','Item_Type','Outlet_Establishment_Year','Outlet_Size','Outlet_Location_Type','Outlet_Type'],prefix = ['Item_Fat_Content','Item_Type','Outlet_Establishment_Year','Outlet_Size','Outlet_Location_Type','Outlet_Type'],drop_first=True).head()

columns= ['Item_Fat_Content','Item_Type','Outlet_Establishment_Year','Outlet_Size','Outlet_Location_Type','Outlet_Type']
test_data = pd.concat([test_data,test_dummies],axis=1)
test_data.drop(columns,axis=1,inplace=True)

test_data.shape

test_data.fillna(0,inplace=True)

"""**9.Final Predictions On The Test Dataset**"""

# Test data on Decision Tree Model

# Predict
test_DT_predictions = DT_model.predict(test_data)

# Validate
print(mean_absolute_error(y_test, test_DT_predictions[:1412]))

# Test data on Random Forest Model

# Predict
test_RF_predictions = RF_model.predict(test_data)

# Validate
print(mean_absolute_error(y_test, test_DT_predictions[:1412]))

# Test data on Linear regression Model

# Predict
test_LR_predictions = LR_model.predict(test_data)

# Validate
print(mean_absolute_error(y_test, test_DT_predictions[:1412]))

# Test data on XG Boost Model

# Predict
test_XG_predictions = XG_model.predict(test_data)

# Validate
print(mean_absolute_error(y_test, test_DT_predictions[:1412]))

"""**10. Saving The Final Model**"""

final = pd.DataFrame(test_XG_predictions)
final.index = test_data.index 
final.columns = ["prediction"]
final.head()

# To download the csv file locally
from google.colab import files
final.to_csv('prediction_results.csv')         
files.download('prediction_results.csv')